---
layout: post
title: "Gorila"
author: Yoshiki
categories: [Research]
image: assets/images/10.jpg
featured: false
hidden: false
wip: true
---

# Massively Parallel Methods for Deep Reinforcement Learning（2015）

Gorila という分散アーキテクチャにより，DQN を複数マシンで大規模並列実行し，学習時間を 1/10 に短縮しつつ性能を向上させた．

## 背景

今までの強化学習は，単一マシンで単一環境において学習を進めていた．しかしこれでは時間がかかる．
Actor と Learner を複数のマシンに分けて同時に動かす Gorila により，早くかつ賢く学習できるようにした．

##

```mermaid
graph TD
    %% Parameter Server 最上部
    P[Parameter Server]

    %% Learners
    L1[Learner 1]
    L2[Learner 2]
    P -->|θ・θ⁻配布| L1
    P -->|θ・θ⁻配布| L2
    L1 -->|勾配| P
    L2 -->|勾配| P

    %% Replay Memories
    R1[Replay Memory 1]
    R2[Replay Memory 2]
    L1 --> R1
    L2 --> R2

    %% Actors
    A1[Actor 1]
    A2[Actor 2]
    A1 --> R1
    A2 --> R2
    P -->|θ配布| A1
    P -->|θ配布| A2
```

```mermaid
graph RL
    subgraph Machine_1
        A1["Actor"] --> R1["Replay Memory"] --> L1["Learner"]
    end
    subgraph Machine_2
        A2 --> R2 --> L2
    end
    L1 --> PS["Parameter Server"]
    L2 --> PS
    PS --> A1
    PS --> A2
```

- Actor：複数動作し，異なる状態を探索．
- Replay Memory：分散か単一かは構成による．
- Learner：DQN の損失で勾配計算し，Parameter Server に送信．
- Parameter Server：全体のパラメータ θ⁺ を保持し，非同期 SGD で更新．

```mermaid
flowchart LR
    subgraph Actor-Machine群
        A1[「Actor 1」] -->|状態・行動・報酬の列| L["キュー"]
        A2[「Actor 2」] -->|状態・行動・報酬の列| L
        A3[「Actor N」] -->|状態・行動・報酬の列| L
        L -->|バッチ学習| B[「Learner」]
        B -->|新しいパラメータ| A1
        B --> A2
        B --> A3
    end
```
